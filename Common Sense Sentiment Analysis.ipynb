{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The words that fueled the American Revolution: Sentiment Analysis on Thomas Paine's _Common Sense_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import urllib.robotparser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# This code checks the robots.txt file\n",
    "def canFetch(url):\n",
    "\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(domain + \"/robots.txt\")\n",
    "    try:\n",
    "        rp.read()\n",
    "        canFetchBool = rp.can_fetch(\"*\", url)\n",
    "    except:\n",
    "        canFetchBool = None\n",
    "    \n",
    "    return canFetchBool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the text file\n",
    "common_sense = [x.strip() for x in open(\"common_sense.txt\", encoding='utf8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Formatting text\n",
    "new_list = []\n",
    "word_by_word = \" \".join(common_sense).split(\" \")\n",
    "# Filtering out short words\n",
    "for word in word_by_word:\n",
    "    if len(word) > 4:\n",
    "        new_list.append(word)\n",
    "no_short_words = \" \".join(new_list)\n",
    "# Cutting out excess text not in actual pamphlet\n",
    "start = 881\n",
    "end = -15841\n",
    "cs = no_short_words[start:end]\n",
    "# Removing punctuation, commas, semicolons, dashes, colons, parenthesis, brackets, extra quotations\n",
    "cs = cs.replace(\",\", \"\").replace(\";\", \"\").replace(\".\", \"\")\\\n",
    ".replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace\\\n",
    "(\":\", \"\").replace(\"--\", \"\").replace(\"?\", \"\").replace(\"!\", \"\")\\\n",
    ".replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', '').replace(\"'\", \"\")\n",
    "# lowercasing all words for simplicity\n",
    "cs = cs.lower() #the whole text with no punctuation or marks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couting Words, Creating a Dictionary, Finding Common Words, and Searching for Particular Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8724, 3130)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting total words\n",
    "cs_list_words = cs.split(\" \") # Splitting list into words\n",
    "total_words = len(cs_list_words) # Finding length of list (total words)\n",
    "# Counting distinct words\n",
    "distinct_split_words = len(set(cs_list_words)) # Filtering list to distinct words\n",
    "total_words, distinct_split_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of words\n",
    "cs_dictionary = {} # empty dictionary\n",
    "for word in cs_list_words:\n",
    "    if word in cs_dictionary and len(word) > 0: # If word is already in dictionary, add one to the count or value\n",
    "        cs_dictionary[word] = cs_dictionary[word] + 1 \n",
    "    elif len(word) > 0:\n",
    "        cs_dictionary[word] = 1 # If word is not already in dictionary, start with value 1\n",
    "#cs_dictionary = a dictionary of every word and its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "being\n",
      "first\n",
      "power\n",
      "which\n",
      "england\n",
      "their\n",
      "america\n",
      "government\n",
      "other\n",
      "without\n",
      "would\n",
      "continent\n"
     ]
    }
   ],
   "source": [
    "# Most common words\n",
    "values = list(cs_dictionary.values()) # List of values\n",
    "keys = list(cs_dictionary.keys()) # List of keys\n",
    "values_sorted = sorted(values, reverse = True) # Sorted values\n",
    "max_values = values_sorted[:10] # Number of times top 10 most common words are used\n",
    "max_values\n",
    "for i in keys:\n",
    "    if cs_dictionary[i] in max_values: # Looping through dictionary to find word used the most\n",
    "        print(i) # The top 10 most common words over 4 letters... that doesn't tell us much!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for how many times particular words are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dictionary[\"power\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dictionary[\"britain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dictionary[\"monarchy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dictionary[\"freedom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dictionary[\"america\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dictionary[\"suffer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dictionary[\"england\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/kjadbaba/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing nltk for Sentiment Analysis\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to hold the sentiment analysis of each word in particular\n",
    "sentiment_list = []\n",
    "for i in keys: # Looping through keys\n",
    "    word_analysis = sid.polarity_scores(i) # sentiment analysis\n",
    "    word_analysis['word'] = i # adding extra category to dictionary\n",
    "    sentiment_list.append(word_analysis) # appending to the list\n",
    "#sentiment_list = a list of dictionaries of the sentiment of every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>-0.7003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>slavery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>-0.6808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>murderer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.6597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>evil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>-0.6597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>devil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>-0.6486</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>murderers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>-0.6486</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>murdering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.6369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>-0.6369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>betray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>-0.6369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hellish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>-0.6369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-0.6249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>-0.6249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>violence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>-0.6124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>brutally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>-0.6124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>murders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>-0.6124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>brutality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      compound  neg  neu  pos       word\n",
       "1285   -0.7003  1.0  0.0  0.0    slavery\n",
       "2177   -0.6808  1.0  0.0  0.0   murderer\n",
       "200    -0.6597  1.0  0.0  0.0       evil\n",
       "747    -0.6597  1.0  0.0  0.0      devil\n",
       "1704   -0.6486  1.0  0.0  0.0  murderers\n",
       "3052   -0.6486  1.0  0.0  0.0  murdering\n",
       "32     -0.6369  1.0  0.0  0.0      abuse\n",
       "1185   -0.6369  1.0  0.0  0.0     betray\n",
       "2123   -0.6369  1.0  0.0  0.0    hellish\n",
       "2738   -0.6369  1.0  0.0  0.0     hatred\n",
       "201    -0.6249  1.0  0.0  0.0      worst\n",
       "1930   -0.6249  1.0  0.0  0.0   violence\n",
       "2131   -0.6124  1.0  0.0  0.0   brutally\n",
       "2164   -0.6124  1.0  0.0  0.0    murders\n",
       "2688   -0.6124  1.0  0.0  0.0  brutality"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cs_ws = pd.DataFrame(sentiment_list) # Making a table\n",
    "worst_words = cs_ws.loc[cs_ws[\"compound\"] <= -.6] # Filtering out compounds greater than -.6\n",
    "worst_words_sort = worst_words.sort_values(\"compound\") # sorting by smallest compound\n",
    "worst_words_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>greatest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>perfectly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>glorious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>splendor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      compound  neg  neu  pos       word\n",
       "220     0.6369  0.0  0.0  1.0   paradise\n",
       "250     0.6369  0.0  0.0  1.0   greatest\n",
       "322     0.6369  0.0  0.0  1.0  perfectly\n",
       "426     0.6369  0.0  0.0  1.0    freedom\n",
       "454     0.6369  0.0  0.0  1.0   glorious\n",
       "1673    0.6369  0.0  0.0  1.0       best\n",
       "1679    0.6369  0.0  0.0  1.0       love\n",
       "95      0.6249  0.0  0.0  1.0      great\n",
       "760     0.6124  0.0  0.0  1.0   splendor"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_words = cs_ws.loc[cs_ws[\"compound\"] >= .6] # Filtering out compounds less than .6\n",
    "best_words_sort = best_words.sort_values(\"compound\", ascending = False) # sorting\n",
    "best_words_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.117, 0.156, 0.728)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = sid.polarity_scores(cs) # Sentiment Analysis on text as a whole (one giant sentence)\n",
    "total_neg = total['neg'] \n",
    "total_pos = total['pos']\n",
    "total_neu = total['neu']\n",
    "(total_neg, total_pos, total_neu) # comparing results of negative vs positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds = list(cs_ws.sort_values('word').loc[: , \"compound\"]) # First sorting the analyzed table (in alphabetical order), then extracting the compound column\n",
    "alphabetical_keys = list(sorted(cs_dictionary.keys())) # Sorting keys of dictionary of words\n",
    "values_sorted = [value for (key, value) in sorted(cs_dictionary.items())] # Sorting values based on sorted keys\n",
    "sorted_dictionary = dict(zip(alphabetical_keys, values_sorted)) # creating a sorted dictionary (mainly to assure everything sorted correctly)\n",
    "dict_values = list(sorted_dictionary.values()) # Values from new dictionary\n",
    "#len(compounds) == len(dict_values) is true\n",
    "products = [a * b for a, b in zip(dict_values, compounds)] # multiplying lists together\n",
    "sum_products = sum(products) # summing together all the multiplied values\n",
    "round(sum_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Overall, a simple sentiment analysis on Thomas Paine's Common Sense can provide a unique understanding on numerous aspects of the pamphlet without reading a single word of it. To begin, I downloaded the text file and cleaned it. Because it is an ebook, it had random text about the website, use-restrictions, and other things that will be found in the first few pages of a book (before the text actually begins). To rid of the excess texts, I manually sliced the string so that it would start with the first word of the text and end with the last word in the actual book. Next, I cut out all words four characters or shorter, lowercased every word, and filtered out any types of punctuation or marks, leaving one long paragraph of every word (or number) separated by a space. After that, I found the amount of words, amount of distinct words, and created a dictionary of every word (key) and how many times that word was used (value). After cleaning the text and creating a dictionary, I analyzed some interesting questions about Common Sense and its overall sentiment.\n",
    "   \n",
    "   First, I found that the most common words over four characters don't display much about the text. The words like \"which\", \"their\", \"other\", and \"could\", for example, are of the top ten most common words, yet are mostly neutral and don't have any significant meaning. On the other hand, some of the other words, like \"government\", \"america\", \"power\", and \"england\" portray how the text has something to do along the lines of government and power in America and England. Second, I wanted to look at the most negative and positive words used by Thomas Paine, and, through his choice of words, try to gain insight on the rhetoric of the text. The negative words, like \"slavery\", \"murderer\", and \"evil\" with the most negative compounds display how Thomas Paine believed the British power and control over America was like slavery, and that he thinks of the British people as evil murderers. In contrast, the positive words, like \"paradise\", \"greatest\", and \"freedom\", are likely used to describe America, the American people, and the potential and future of America if the country become free from England's control. Through this simple sentiment analysis of the individual words in Common Sense, we can see how Thomas Paine was trying to persuade the Americans to stand up against the power of the British, and specifically what words he used to accomplish that goal. Still, this analysis is limited as the words are out of context, and does not give a full conlusion on the overall sentiment of the text. To accomplish that question, I used two different methods. In the first method, I ran the sentiment analyzer on the whole text as one large sentence, and found that it is slightly more positive than negative, but more neutral than anything. The fact that most of the words are neutral makes sense in that the analyzer takes each word at a time, so most words in any text will most probably be neutral. To validate this first method, I sought a way to express the sentiment as a value; the more positive the value, the more positive the overall sentiment of the text, and vice versa. To do this, I took the sum of each compound multiplied by the amount of times the word was used. For example, the sentence \"bad bad bad good good\" will be represented by a negative value because the compound of bad will be multipled by three and good by two. At the same time, a word that is more on the ends of the spectrum, like \"slavery\" will have a larger impact on the value than a word like \"bad\". As another example, the sentence \"slavery slavery good good good\" could be positive or negative, depending on the compounds of each word. As a whole, using this second method, I found that the value rounded to the nearest whole number is 83, so it can be concluded, by the consistency of both methods, that the overall sentiment is more positive than negative. Furthermore, a lot of information on Thomas Paine's Common Sense can be gathered and understood through all of the unique analysis completed. While there are many limitations to the analysis including the accuracy of the sentiment analyzer (for example, \"not good\" will be seen as positive), we can see that Thomas Paine's overall sentiment was positive, which could imply that he focused on praising America and uniting and galvanizing the people on the greatness of the country, the people, and its need for independence from England (my interpretation from the analysis). In conclusion, this sentiment analysis on Thomas Paine's Common Sense demonstrates many interesting aspects and statistics about the text which can be understood without reading it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
